<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of mlpfwd</title>
  <meta name="keywords" content="mlpfwd">
  <meta name="description" content="MLPFWD	Forward propagation through 2-layer network.">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html &copy; 2003 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../menu.html">Home</a> &gt;  <a href="#">ReBEL-0.2.7</a> &gt; <a href="#">netlab</a> &gt; mlpfwd.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../menu.html"><img alt="<" border="0" src="../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="menu.html">Index for .\ReBEL-0.2.7\netlab&nbsp;<img alt=">" border="0" src="../../right.png"></a></td></tr></table>-->

<h1>mlpfwd
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>MLPFWD	Forward propagation through 2-layer network.</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function [y, z, a] = mlpfwd(net, x) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre class="comment">MLPFWD    Forward propagation through 2-layer network.

    Description
    Y = MLPFWD(NET, X) takes a network data structure NET together with a
    matrix X of input vectors, and forward propagates the inputs through
    the network to generate a matrix Y of output vectors. Each row of X
    corresponds to one input vector and each row of Y corresponds to one
    output vector.

    [Y, Z] = MLPFWD(NET, X) also generates a matrix Z of the hidden unit
    activations where each row corresponds to one pattern.

    [Y, Z, A] = MLPFWD(NET, X) also returns a matrix A  giving the summed
    inputs to each output unit, where each row corresponds to one
    pattern.

    See also
    <a href="mlp.html" class="code" title="function net = mlp(nin, nhidden, nout, outfunc, prior, beta)">MLP</a>, <a href="mlppak.html" class="code" title="function w = mlppak(net)">MLPPAK</a>, <a href="mlpunpak.html" class="code" title="function net = mlpunpak(net, w)">MLPUNPAK</a>, <a href="mlperr.html" class="code" title="function [e, edata, eprior, mse] = mlperr(net, x, t)">MLPERR</a>, <a href="mlpbkp.html" class="code" title="function g = mlpbkp(net, x, z, deltas)">MLPBKP</a>, <a href="mlpgrad.html" class="code" title="function [g, gdata, gprior] = mlpgrad(net, x, t)">MLPGRAD</a></pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="consist.html" class="code" title="function errstring = consist(model, type, inputs, outputs)">consist</a>	CONSIST Check that arguments are consistent.</li></ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="demard.html" class="code" title="">demard</a>	DEMARD	Automatic relevance determination using the MLP.</li><li><a href="demev1.html" class="code" title="">demev1</a>	DEMEV1	Demonstrate Bayesian regression for the MLP.</li><li><a href="demev2.html" class="code" title="">demev2</a>	DEMEV2	Demonstrate Bayesian classification for the MLP.</li><li><a href="demhmc2.html" class="code" title="">demhmc2</a>	DEMHMC2 Demonstrate Bayesian regression with Hybrid Monte Carlo sampling.</li><li><a href="demhmc3.html" class="code" title="">demhmc3</a>	DEMHMC3 Demonstrate Bayesian regression with Hybrid Monte Carlo sampling.</li><li><a href="demmdn1.html" class="code" title="">demmdn1</a>	DEMMDN1 Demonstrate fitting a multi-valued function using a Mixture Density Network.</li><li><a href="demmlp1.html" class="code" title="">demmlp1</a>	DEMMLP1 Demonstrate simple regression using a multi-layer perceptron</li><li><a href="demmlp2.html" class="code" title="">demmlp2</a>	DEMMLP2 Demonstrate simple classification using a multi-layer perceptron</li><li><a href="demolgd1.html" class="code" title="">demolgd1</a>	DEMOLGD1 Demonstrate simple MLP optimisation with on-line gradient descent</li><li><a href="demprior.html" class="code" title="function demprior(action);">demprior</a>	DEMPRIOR Demonstrate sampling from a multi-parameter Gaussian prior.</li><li><a href="demtrain.html" class="code" title="function demtrain(action);">demtrain</a>	DEMTRAIN Demonstrate training of MLP network.</li><li><a href="mdnfwd.html" class="code" title="function [mixparams, y, z, a] = mdnfwd(net, x)">mdnfwd</a>	MDNFWD	Forward propagation through Mixture Density Network.</li><li><a href="mlpderiv.html" class="code" title="function g = mlpderiv(net, x)">mlpderiv</a>	MLPDERIV Evaluate derivatives of network outputs with respect to weights.</li><li><a href="mlperr.html" class="code" title="function [e, edata, eprior, mse] = mlperr(net, x, t)">mlperr</a>	MLPERR Evaluate error function for 2-layer network.</li><li><a href="mlpevfwd.html" class="code" title="function [y, extra, invhess] = mlpevfwd(net, x, t, x_test, invhess)">mlpevfwd</a>	MLPEVFWD Forward propagation with evidence for MLP</li><li><a href="mlpgrad.html" class="code" title="function [g, gdata, gprior] = mlpgrad(net, x, t)">mlpgrad</a>	MLPGRAD Evaluate gradient of error function for 2-layer network.</li><li><a href="mlphdotv.html" class="code" title="function hdv = mlphdotv(net, x, t, v)">mlphdotv</a>	MLPHDOTV Evaluate the product of the data Hessian with a vector.</li></ul>
<!-- crossreference -->


<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [y, z, a] = mlpfwd(net, x)</a>
0002 <span class="comment">%MLPFWD    Forward propagation through 2-layer network.</span>
0003 <span class="comment">%</span>
0004 <span class="comment">%    Description</span>
0005 <span class="comment">%    Y = MLPFWD(NET, X) takes a network data structure NET together with a</span>
0006 <span class="comment">%    matrix X of input vectors, and forward propagates the inputs through</span>
0007 <span class="comment">%    the network to generate a matrix Y of output vectors. Each row of X</span>
0008 <span class="comment">%    corresponds to one input vector and each row of Y corresponds to one</span>
0009 <span class="comment">%    output vector.</span>
0010 <span class="comment">%</span>
0011 <span class="comment">%    [Y, Z] = MLPFWD(NET, X) also generates a matrix Z of the hidden unit</span>
0012 <span class="comment">%    activations where each row corresponds to one pattern.</span>
0013 <span class="comment">%</span>
0014 <span class="comment">%    [Y, Z, A] = MLPFWD(NET, X) also returns a matrix A  giving the summed</span>
0015 <span class="comment">%    inputs to each output unit, where each row corresponds to one</span>
0016 <span class="comment">%    pattern.</span>
0017 <span class="comment">%</span>
0018 <span class="comment">%    See also</span>
0019 <span class="comment">%    MLP, MLPPAK, MLPUNPAK, MLPERR, MLPBKP, MLPGRAD</span>
0020 <span class="comment">%</span>
0021 
0022 <span class="comment">%    Copyright (c) Ian T Nabney (1996-2001)</span>
0023 
0024 <span class="comment">% Check arguments for consistency</span>
0025 errstring = <a href="consist.html" class="code" title="function errstring = consist(model, type, inputs, outputs)">consist</a>(net, <span class="string">'mlp'</span>, x);
0026 <span class="keyword">if</span> ~isempty(errstring);
0027   error(errstring);
0028 <span class="keyword">end</span>
0029 
0030 ndata = size(x, 1);
0031 
0032 z = tanh(x*net.w1 + ones(ndata, 1)*net.b1);
0033 a = z*net.w2 + ones(ndata, 1)*net.b2;
0034 
0035 <span class="keyword">switch</span> net.outfn
0036 
0037   <span class="keyword">case</span> <span class="string">'linear'</span>    <span class="comment">% Linear outputs</span>
0038 
0039     y = a;
0040 
0041   <span class="keyword">case</span> <span class="string">'logistic'</span>  <span class="comment">% Logistic outputs</span>
0042     <span class="comment">% Prevent overflow and underflow: use same bounds as mlperr</span>
0043     <span class="comment">% Ensure that log(1-y) is computable: need exp(a) &gt; eps</span>
0044     maxcut = -log(eps);
0045     <span class="comment">% Ensure that log(y) is computable</span>
0046     mincut = -log(1/realmin - 1);
0047     a = min(a, maxcut);
0048     a = max(a, mincut);
0049     y = 1./(1 + exp(-a));
0050 
0051   <span class="keyword">case</span> <span class="string">'softmax'</span>   <span class="comment">% Softmax outputs</span>
0052   
0053     <span class="comment">% Prevent overflow and underflow: use same bounds as glmerr</span>
0054     <span class="comment">% Ensure that sum(exp(a), 2) does not overflow</span>
0055     maxcut = log(realmax) - log(net.nout);
0056     <span class="comment">% Ensure that exp(a) &gt; 0</span>
0057     mincut = log(realmin);
0058     a = min(a, maxcut);
0059     a = max(a, mincut);
0060     temp = exp(a);
0061     y = temp./(sum(temp, 2)*ones(1, net.nout));
0062 
0063   <span class="keyword">otherwise</span>
0064     error([<span class="string">'Unknown activation function '</span>, net.outfn]);  
0065 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Tue 26-Sep-2006 10:36:21 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/">m2html</a></strong> &copy; 2003</address>
</body>
</html>